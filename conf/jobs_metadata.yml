# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  examples/ex0_extraction_job.py:
    description: "Sample API extraction job, pulling public wikipedia data."
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{base_path}/wiki_example/input/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    frequency: '@once'
    spark_boot: False

  bts/ex1_datadup_job.py:
    description: "dup data 100x"
    inputs:
      pageviews: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/bts/ex1_datadup_job/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    # no_fw_cache: True
    ec2_instance_master: 'm5.xlarge'
    ec2_instance_slaves: 'm5.xlarge'
    emr_core_instances: 2

  bts/ex1_page_metrics_job.sql:
    description: "shows sql job, using spark, for large datasets."
    py_job: 'jobs/generic/sql_spark_job.py'
    # sql_file: 'jobs/bts/ex1_user_metrics_job.sql'
    inputs:
      pageviews: {'path':"{base_path}/wiki_example/bts/ex1_datadup_job/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/bts/ex1_page_metrics_job/{now}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    repartition: 1
    frequency: '@once'
    emails: ['some_email@address.com']


  # ----- Climate data (Carbon emissions) Jobs --------
  climate/climate_trace_extraction_job.py:
    description: "Ingest climate trace data. From https://climatetrace.org/."
    output: {'path':'{base_path}/climate_trace/raw/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False

  climate/climate_trace_asset_dimension_job.py:
    description: "Get info about emitting assets from climate trace data."
    inputs:
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/asset_dimension/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies: [climate/climate_trace_extraction_job.py]

  climate/climate_trace_emission_fact_job.py:
    description: "Get yearly emission data from asset and emission type in climate trace data."
    inputs:
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/emission_facts/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies: [climate/climate_trace_extraction_job.py]

  climate/run_all:
    description: "Run all climate-trace jobs"
    py_job: jobs/generic/dummy_job.py
    spark_boot: False
    dependencies: 
      - climate/climate_trace_extraction_job.py
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py


# ----- Params -------
common_params:
  all_mode_params:
    base_path: '{root_path}/pipelines_data'  # don't add '/' at the end
    s3_dags: '{root_path}/pipelines_metadata/airflow_dags'  # determines which airflow instance to use.
    s3_logs: '{root_path}/pipelines_metadata'
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.5' # options: '2.4', '3.0', '3.4' or '3.5'
  mode_specific_params:
    prod_EMR:
      root_path: s3://mylake-prod  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        pro
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      # root_path: s3://mylake-dev  # don't add '/' at the end
      root_path: s3://mylake-dev-testap  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev_AWSAcad
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      root_path: '.'  # don't add '/' at the end
      base_path: '{root_path}/data'  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_db_push: False
      save_schemas: True
      manage_git_info: False
